{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "projected_gradient_descent.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true,
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/nowei/adversarial-dl/blob/master/projected_gradient_descent.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MOjcxt-MTkw5",
        "colab_type": "text"
      },
      "source": [
        "### This notebook is optionally accelerated with a GPU runtime.\n",
        "### If you would like to use this acceleration, please select the menu option \"Runtime\" -> \"Change runtime type\", select \"Hardware Accelerator\" -> \"GPU\" and click \"SAVE\"\n",
        "\n",
        "----------------------------------------------------------------------\n",
        "\n",
        "# ResNet\n",
        "\n",
        "*Author: Pytorch Team*\n",
        "\n",
        "**Deep residual networks pre-trained on ImageNet**\n",
        "\n",
        "<img src=\"https://pytorch.org/assets/images/resnet.png\" alt=\"alt\" width=\"50%\"/>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SpN_KNKuTkw7",
        "colab_type": "code",
        "outputId": "ae104801-ab01-4149-f91c-975ddc08a821",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "import torch\n",
        "# model = torch.hub.load('pytorch/vision:v0.4.2', 'resnet18', pretrained=True)\n",
        "# or any of these variants\n",
        "# model = torch.hub.load('pytorch/vision:v0.4.2', 'resnet34', pretrained=True)\n",
        "model = torch.hub.load('pytorch/vision:v0.4.2', 'resnet50', pretrained=True)\n",
        "# model = torch.hub.load('pytorch/vision:v0.4.2', 'resnet101', pretrained=True)\n",
        "# model = torch.hub.load('pytorch/vision:v0.4.2', 'resnet152', pretrained=True)\n",
        "model.eval()"
      ],
      "execution_count": 65,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Using cache found in /root/.cache/torch/hub/pytorch_vision_v0.4.2\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "ResNet(\n",
              "  (conv1): Conv2d(3, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False)\n",
              "  (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "  (relu): ReLU(inplace=True)\n",
              "  (maxpool): MaxPool2d(kernel_size=3, stride=2, padding=1, dilation=1, ceil_mode=False)\n",
              "  (layer1): Sequential(\n",
              "    (0): Bottleneck(\n",
              "      (conv1): Conv2d(64, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "      (bn3): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (relu): ReLU(inplace=True)\n",
              "      (downsample): Sequential(\n",
              "        (0): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "        (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      )\n",
              "    )\n",
              "    (1): Bottleneck(\n",
              "      (conv1): Conv2d(256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "      (bn3): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (relu): ReLU(inplace=True)\n",
              "    )\n",
              "    (2): Bottleneck(\n",
              "      (conv1): Conv2d(256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "      (bn3): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (relu): ReLU(inplace=True)\n",
              "    )\n",
              "  )\n",
              "  (layer2): Sequential(\n",
              "    (0): Bottleneck(\n",
              "      (conv1): Conv2d(256, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
              "      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "      (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (relu): ReLU(inplace=True)\n",
              "      (downsample): Sequential(\n",
              "        (0): Conv2d(256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
              "        (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      )\n",
              "    )\n",
              "    (1): Bottleneck(\n",
              "      (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "      (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (relu): ReLU(inplace=True)\n",
              "    )\n",
              "    (2): Bottleneck(\n",
              "      (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "      (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (relu): ReLU(inplace=True)\n",
              "    )\n",
              "    (3): Bottleneck(\n",
              "      (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "      (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (relu): ReLU(inplace=True)\n",
              "    )\n",
              "  )\n",
              "  (layer3): Sequential(\n",
              "    (0): Bottleneck(\n",
              "      (conv1): Conv2d(512, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
              "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (relu): ReLU(inplace=True)\n",
              "      (downsample): Sequential(\n",
              "        (0): Conv2d(512, 1024, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
              "        (1): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      )\n",
              "    )\n",
              "    (1): Bottleneck(\n",
              "      (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (relu): ReLU(inplace=True)\n",
              "    )\n",
              "    (2): Bottleneck(\n",
              "      (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (relu): ReLU(inplace=True)\n",
              "    )\n",
              "    (3): Bottleneck(\n",
              "      (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (relu): ReLU(inplace=True)\n",
              "    )\n",
              "    (4): Bottleneck(\n",
              "      (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (relu): ReLU(inplace=True)\n",
              "    )\n",
              "    (5): Bottleneck(\n",
              "      (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (relu): ReLU(inplace=True)\n",
              "    )\n",
              "  )\n",
              "  (layer4): Sequential(\n",
              "    (0): Bottleneck(\n",
              "      (conv1): Conv2d(1024, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
              "      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (conv3): Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "      (bn3): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (relu): ReLU(inplace=True)\n",
              "      (downsample): Sequential(\n",
              "        (0): Conv2d(1024, 2048, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
              "        (1): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      )\n",
              "    )\n",
              "    (1): Bottleneck(\n",
              "      (conv1): Conv2d(2048, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (conv3): Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "      (bn3): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (relu): ReLU(inplace=True)\n",
              "    )\n",
              "    (2): Bottleneck(\n",
              "      (conv1): Conv2d(2048, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (conv3): Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "      (bn3): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (relu): ReLU(inplace=True)\n",
              "    )\n",
              "  )\n",
              "  (avgpool): AdaptiveAvgPool2d(output_size=(1, 1))\n",
              "  (fc): Linear(in_features=2048, out_features=1000, bias=True)\n",
              ")"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 65
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5o-n_QMqTkw_",
        "colab_type": "text"
      },
      "source": [
        "All pre-trained models expect input images normalized in the same way,\n",
        "i.e. mini-batches of 3-channel RGB images of shape `(3 x H x W)`, where `H` and `W` are expected to be at least `224`.\n",
        "The images have to be loaded in to a range of `[0, 1]` and then normalized using `mean = [0.485, 0.456, 0.406]`\n",
        "and `std = [0.229, 0.224, 0.225]`.\n",
        "\n",
        "Here's a sample execution."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "H2Jzps2KabIp",
        "colab_type": "code",
        "outputId": "fde168be-6d8c-43fe-b374-5eb8d4631fe4",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 204
        }
      },
      "source": [
        "!wget https://gist.githubusercontent.com/yrevar/942d3a0ac09ec9e5eb3a/raw/238f720ff059c1f82f368259d1ca4ffa5dd8f9f5/imagenet1000_clsidx_to_labels.txt\n",
        "with open('imagenet1000_clsidx_to_labels.txt', 'r') as f:\n",
        "    classes = eval(f.read())"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "--2019-11-24 22:18:17--  https://gist.githubusercontent.com/yrevar/942d3a0ac09ec9e5eb3a/raw/238f720ff059c1f82f368259d1ca4ffa5dd8f9f5/imagenet1000_clsidx_to_labels.txt\n",
            "Resolving gist.githubusercontent.com (gist.githubusercontent.com)... 151.101.0.133, 151.101.64.133, 151.101.128.133, ...\n",
            "Connecting to gist.githubusercontent.com (gist.githubusercontent.com)|151.101.0.133|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 30564 (30K) [text/plain]\n",
            "Saving to: ‘imagenet1000_clsidx_to_labels.txt’\n",
            "\n",
            "\r          imagenet1   0%[                    ]       0  --.-KB/s               \rimagenet1000_clsidx 100%[===================>]  29.85K  --.-KB/s    in 0.01s   \n",
            "\n",
            "2019-11-24 22:18:18 (2.43 MB/s) - ‘imagenet1000_clsidx_to_labels.txt’ saved [30564/30564]\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FvPfxRF8TkxA",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Download an example image from the pytorch website\n",
        "import urllib\n",
        "url, filename = (\"https://pbs.twimg.com/media/DzIyOyEWwAAOxWv.jpg\", \"dog.jpg\")\n",
        "try: urllib.URLopener().retrieve(url, filename)\n",
        "except: urllib.request.urlretrieve(url, filename)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zDHln_HXT4q4",
        "colab_type": "code",
        "outputId": "de99c1cd-0320-4260-fac3-4f1ff7887c10",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 241
        }
      },
      "source": [
        "from IPython.display import Image as imshow\n",
        "imshow(filename)\n"
      ],
      "execution_count": 72,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "image/jpeg": "/9j/4AAQSkZJRgABAQAAAQABAAD/2wBDAAUDBAQEAwUEBAQFBQUGBwwIBwcHBw8LCwkMEQ8SEhEP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\nsLe0PQKLrv7KdDpUNsCicofhHiPM4R4gZUrCBSd8ygruY/eIVOyNT3c//9k=\n",
            "text/plain": [
              "<IPython.core.display.Image object>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 72
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qxo6wXXnCpRc",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# sample execution (requires torchvision)\n",
        "from PIL import Image\n",
        "from torchvision import transforms\n",
        "import torch.nn.functional as F\n",
        "import torch\n",
        "input_image = Image.open(filename)\n",
        "preprocess = transforms.Compose([\n",
        "    transforms.Resize(256),\n",
        "    transforms.CenterCrop(224),\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n",
        "])\n",
        "input_tensor = preprocess(input_image)\n",
        "data = input_tensor.unsqueeze(0) # create a mini-batch as expected by the model"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "f6-82CGVTkxC",
        "colab_type": "code",
        "outputId": "aa63f18e-1b72-42a6-ed8c-aa9d33a7f96b",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 68
        }
      },
      "source": [
        "\n",
        "\n",
        "def fgsm_attack(image, epsilon, data_graph):\n",
        "    sign_data_grad = data_grad.sign()\n",
        "    perturbed_image = image + epsilon*sign_data_grad\n",
        "    perturbed_image = torch.clamp(perturbed_image, 0, 1)\n",
        "    print('changed', torch.sum(epsilon*sign_data_grad))\n",
        "    return perturbed_image\n",
        "\n",
        "epsilon = 20/255\n",
        "target = torch.zeros(1,dtype=torch.long)\n",
        "target[0] = 1\n",
        "device = 'cuda'\n",
        "# move the input and model to GPU for speed if available\n",
        "if torch.cuda.is_available():\n",
        "    data = data.to(device)\n",
        "    model.to(device)\n",
        "    # Send the data and label to the device\n",
        "    target = target.to(device)\n",
        "\n",
        "    # Set requires_grad attribute of tensor. Important for Attack\n",
        "    data.requires_grad = True\n",
        "    \n",
        "    # Forward pass the data through the model\n",
        "    output = model(data)\n",
        "    init_pred = output.max(1, keepdim=True)[1] # get the index of the max log-probability\n",
        "    print(classes[init_pred.item()], torch.nn.functional.softmax(output[0], dim=0).max())\n",
        "    # Calculate the loss\n",
        "    loss = F.nll_loss(output, target)\n",
        "    \n",
        "    # Zero all existing gradients\n",
        "    model.zero_grad()\n",
        "\n",
        "    # Calculate gradients of model in backward pass\n",
        "    loss.backward()\n",
        "\n",
        "    # Collect datagrad\n",
        "    data_grad = data.grad.data\n",
        "\n",
        "    # Call FGSM Attack\n",
        "    perturbed_data = fgsm_attack(data, epsilon, data_grad)\n",
        "\n",
        "    # Re-classify the perturbed image\n",
        "    output = model(perturbed_data)\n",
        "    print('perturbedddddd')\n",
        "# with torch.no_grad():\n",
        "    # output = model(input_batch)\n",
        "# Tensor of shape 1000, with confidence scores over Imagenet's 1000 classes\n",
        "# The output has unnormalized scores. To get probabilities, you can run a softmax on it.\n",
        "probs = torch.nn.functional.softmax(output[0], dim=0)"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "giant panda, panda, panda bear, coon bear, Ailuropoda melanoleuca tensor(0.8202, device='cuda:0', grad_fn=<MaxBackward1>)\n",
            "changed tensor(30.9020, device='cuda:0')\n",
            "perturbedddddd\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DDIwjtaHTBKN",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def proj_grad_desc(x, y, model, step_size=0.01, epsilon=4 * 1/255, steps=50, target=None):\n",
        "    adversary = x.clone().detach().requires_grad_(True).to(x.device)\n",
        "    max_diff = x + epsilon\n",
        "    min_diff = x - epsilon\n",
        "    for i in range(steps):\n",
        "        curr = adversary.clone().detach().requires_grad_(True).to(adversary.device)\n",
        "        output = model(curr)\n",
        "        loss = F.nll_loss(output, target if target else y)\n",
        "        loss.backward()\n",
        "        with torch.no_grad():\n",
        "            curr_grad = curr.grad * step_size \n",
        "            if target:\n",
        "                adversary -= curr_grad\n",
        "            else:\n",
        "                adversary += curr_grad\n",
        "        adversary = torch.max(torch.min(adversary, max_diff), min_diff)\n",
        "        # adversary = adversary.clamp(0, 1)\n",
        "    return adversary.detach()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "be63gDP7tb0d",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "trans = transforms.ToPILImage()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gULvyrpt4anC",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "preprocess_2 = transforms.Compose([\n",
        "    transforms.Resize(256),\n",
        "    transforms.CenterCrop(224),\n",
        "    transforms.ToTensor(),\n",
        "])\n",
        "preprocess_3 = transforms.Compose([\n",
        "    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])                                   \n",
        "])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "a8vJjNFcnQcp",
        "colab_type": "code",
        "outputId": "dec2f316-af0a-46f5-d73c-1c62fbf9bc59",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 409
        }
      },
      "source": [
        "label = torch.tensor([388]).to(data.device)\n",
        "print(classes[label.item()])\n",
        "target = torch.tensor([368]).to(data.device)\n",
        "# target = None\n",
        "if target:\n",
        "    print(classes[target.item()])\n",
        "adv = proj_grad_desc(data, label, model, 0.01, 4 * 1/255, steps=50, target=target)\n",
        "print('nani')\n",
        "# curr = preprocess_3(preprocess_2(input_image) + (data - adv).to('cpu'))\n",
        "# print(curr)\n",
        "# print(classes[model(curr.to(data.device)).argmax().item()])\n",
        "# trans(curr[0].to('cpu')).save('nani.jpg')\n",
        "trans(input_tensor.to('cpu')).save('nanie.jpg')\n",
        "result = model(adv)\n",
        "probs = torch.nn.functional.softmax(result[0], dim=0)\n",
        "adv[:, 0, :, :] = adv[:, 0, :, :] * 0.229 + 0.485\n",
        "adv[:, 1, :, :] = adv[:, 1, :, :] * 0.224 + 0.456\n",
        "adv[:, 2, :, :] = adv[:, 2, :, :] * 0.225 + 0.406\n",
        "print(classes[probs.argmax().item()], probs.max())\n",
        "trans(adv.to('cpu')[0]).save('pgd_doggo.jpg')\n",
        "adv[:, 0, :, :] = (adv[:, 0, :, :] - 0.485) / 0.229\n",
        "adv[:, 1, :, :] = (adv[:, 1, :, :] - 0.456) / 0.224 \n",
        "adv[:, 2, :, :] = (adv[:, 2, :, :] - 0.406) / 0.225\n",
        "# trans(adv.to('cpu')[0]).save('pgd_doggo.jpg')\n",
        "print(adv)\n",
        "result = model(adv)\n",
        "probs = torch.nn.functional.softmax(result[0], dim=0)\n",
        "print(classes[probs.argmax().item()], probs.max())\n"
      ],
      "execution_count": 76,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "giant panda, panda, panda bear, coon bear, Ailuropoda melanoleuca\n",
            "gibbon, Hylobates lar\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-76-824f58787b1c>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mtarget\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mclasses\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mtarget\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitem\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 7\u001b[0;31m \u001b[0madv\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mproj_grad_desc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0.01\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m4\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m/\u001b[0m\u001b[0;36m255\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msteps\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m50\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtarget\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      8\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'nani'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0;31m# curr = preprocess_3(preprocess_2(input_image) + (data - adv).to('cpu'))\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-66-c6e624db8243>\u001b[0m in \u001b[0;36mproj_grad_desc\u001b[0;34m(x, y, model, step_size, epsilon, steps, target)\u001b[0m\n\u001b[1;32m      7\u001b[0m         \u001b[0moutput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcurr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m         \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mF\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnll_loss\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mtarget\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 9\u001b[0;31m         \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     10\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mno_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m             \u001b[0mcurr_grad\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcurr\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgrad\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mstep_size\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/torch/tensor.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(self, gradient, retain_graph, create_graph)\u001b[0m\n\u001b[1;32m    164\u001b[0m                 \u001b[0mproducts\u001b[0m\u001b[0;34m.\u001b[0m \u001b[0mDefaults\u001b[0m \u001b[0mto\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    165\u001b[0m         \"\"\"\n\u001b[0;32m--> 166\u001b[0;31m         \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mautograd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgradient\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    167\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    168\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mregister_hook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/torch/autograd/__init__.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables)\u001b[0m\n\u001b[1;32m     97\u001b[0m     Variable._execution_engine.run_backward(\n\u001b[1;32m     98\u001b[0m         \u001b[0mtensors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgrad_tensors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 99\u001b[0;31m         allow_unreachable=True)  # allow_unreachable flag\n\u001b[0m\u001b[1;32m    100\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    101\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HjiudlT9DHNP",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 136
        },
        "outputId": "f8c11900-8eb2-4749-ff41-f9079a190ebf"
      },
      "source": [
        "!ps"
      ],
      "execution_count": 81,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "    PID TTY          TIME CMD\n",
            "      1 ?        00:00:00 run.sh\n",
            "      9 ?        00:00:06 node\n",
            "     24 ?        00:00:11 jupyter-noteboo\n",
            "    114 ?        00:00:00 tail\n",
            "    122 ?        00:01:48 python3\n",
            "   1281 ?        00:00:00 ps\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "h67FESc5UF2P",
        "colab_type": "code",
        "outputId": "a26c46fb-8183-4189-dcaf-8cf6217ec1d2",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "print(classes[probs.argmax().item()])"
      ],
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "gibbon, Hylobates lar\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aGl6XTOFZy6R",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "local = trans(perturbed_data.to('cpu')[0])\n",
        "trans(data.to('cpu')[0]).save('orig_doggo.jpg')\n",
        "local.save('doggo.jpg')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RaHK0EGNiqLE",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from torch.utils.data import DataLoader\n",
        "from torchvision import datasets\n",
        "import torch.optim as optim\n",
        "import torch.nn as nn\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BdUZOzW5iLRB",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "d9c2b17c-86a0-43f5-fe89-6ba5fb21e77e"
      },
      "source": [
        "#training\n",
        "# train = datasets.ImageNet('./data/', train=True, transform=preprocess, download=True)\n",
        "# train_loader = DataLoader(train, batch_size=128)\n",
        "model_training = torch.hub.load('pytorch/vision:v0.4.2', 'resnet50', pretrained=True)\n",
        "train_loader = torch.utils.data.DataLoader(data_train, batch_size=128,\\\n",
        "                                           shuffle=True)\n",
        "test_loader = torch.utils.data.DataLoader(data_test, batch_size=128, \\\n",
        "                                          shuffle=False)\n",
        "model_training.to(device)\n",
        "model_training.train()\n",
        "optimizer = optim.SGD(model.parameters(), lr=0.01)\n",
        "loss_fn = nn.CrossEntropyLoss()\n",
        "\n",
        "for epoch in range(50):\n",
        "    model.train()\n",
        "    for x, y in train_loader:\n",
        "        x = x.to(device)\n",
        "        y = y.to(device)\n",
        "        print(y)\n",
        "        x_adv = proj_grad_desc(x, y, model, step_size=0.01, epsilon=4 * 1/255)\n",
        "        optimizer.zero_grad()\n",
        "        y_pred = model_training(x_adv)\n",
        "        loss = loss_fn(y_pred, y)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "    print(\"epoch {}: loss {}\".format(epoch, loss.item()))\n",
        "    model.eval()\n",
        "    correct = 0\n",
        "    with torch.no_grad():\n",
        "        for x, y in test_loader:\n",
        "            x_adv = proj_grad_desc(x, y, model, step_size=0.01, epsilon=4 * 1/255)\n",
        "            optimizer.zero_grad()\n",
        "            y_pred = model_training(x_adv)\n",
        "            test_loss_on = F.cross_entropy(output, label.squeeze(), reduction='elementwise_mean')\n",
        "            test_loss += test_loss_on\n",
        "            pred = output.max(1)[1]\n",
        "            correct_mask = pred.eq(label.view_as(pred))\n",
        "            num_correct = correct_mask.sum().item()\n",
        "            correct += num_correct\n",
        "    test_accuracy = 100. * correct / len(test_loader.dataset)\n",
        "    print(\"epoch {}: {}% accuracy\".format(epoch, test_accuracy*100))"
      ],
      "execution_count": 63,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Using cache found in /root/.cache/torch/hub/pytorch_vision_v0.4.2\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "tensor([[ 37],\n",
            "        [153],\n",
            "        [106],\n",
            "        [173],\n",
            "        [175],\n",
            "        [134],\n",
            "        [145],\n",
            "        [195],\n",
            "        [178],\n",
            "        [183],\n",
            "        [161],\n",
            "        [107],\n",
            "        [187],\n",
            "        [111],\n",
            "        [191],\n",
            "        [184],\n",
            "        [ 95],\n",
            "        [ 19],\n",
            "        [ 32],\n",
            "        [103],\n",
            "        [196],\n",
            "        [129],\n",
            "        [ 18],\n",
            "        [ 59],\n",
            "        [ 13],\n",
            "        [ 97],\n",
            "        [197],\n",
            "        [122],\n",
            "        [ 37],\n",
            "        [ 63],\n",
            "        [ 83],\n",
            "        [123],\n",
            "        [ 51],\n",
            "        [117],\n",
            "        [  3],\n",
            "        [  0],\n",
            "        [ 83],\n",
            "        [185],\n",
            "        [ 18],\n",
            "        [162],\n",
            "        [ 44],\n",
            "        [191],\n",
            "        [161],\n",
            "        [108],\n",
            "        [ 81],\n",
            "        [ 17],\n",
            "        [ 34],\n",
            "        [ 82],\n",
            "        [ 97],\n",
            "        [167],\n",
            "        [ 23],\n",
            "        [ 97],\n",
            "        [185],\n",
            "        [ 98],\n",
            "        [ 40],\n",
            "        [153],\n",
            "        [ 56],\n",
            "        [ 31],\n",
            "        [ 65],\n",
            "        [185],\n",
            "        [ 60],\n",
            "        [ 76],\n",
            "        [171],\n",
            "        [ 87],\n",
            "        [182],\n",
            "        [ 25],\n",
            "        [149],\n",
            "        [173],\n",
            "        [ 24],\n",
            "        [ 57],\n",
            "        [159],\n",
            "        [124],\n",
            "        [109],\n",
            "        [ 48],\n",
            "        [141],\n",
            "        [ 80],\n",
            "        [173],\n",
            "        [ 48],\n",
            "        [ 39],\n",
            "        [ 21],\n",
            "        [141],\n",
            "        [132],\n",
            "        [ 11],\n",
            "        [129],\n",
            "        [ 60],\n",
            "        [100],\n",
            "        [186],\n",
            "        [100],\n",
            "        [198],\n",
            "        [150],\n",
            "        [130],\n",
            "        [164],\n",
            "        [ 20],\n",
            "        [132],\n",
            "        [  2],\n",
            "        [ 97],\n",
            "        [ 66],\n",
            "        [101],\n",
            "        [ 47],\n",
            "        [147],\n",
            "        [  4],\n",
            "        [  0],\n",
            "        [175],\n",
            "        [ 23],\n",
            "        [ 33],\n",
            "        [181],\n",
            "        [111],\n",
            "        [121],\n",
            "        [  9],\n",
            "        [171],\n",
            "        [124],\n",
            "        [137],\n",
            "        [ 18],\n",
            "        [ 46],\n",
            "        [157],\n",
            "        [ 34],\n",
            "        [ 66],\n",
            "        [ 72],\n",
            "        [158],\n",
            "        [ 63],\n",
            "        [180],\n",
            "        [ 10],\n",
            "        [ 51],\n",
            "        [ 23],\n",
            "        [157],\n",
            "        [ 75],\n",
            "        [ 53],\n",
            "        [144]], device='cuda:0')\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "error",
          "ename": "RuntimeError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-63-0578836579c8>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     13\u001b[0m         \u001b[0my\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m         \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 15\u001b[0;31m         \u001b[0mx_adv\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mproj_grad_desc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstep_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0.01\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepsilon\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m4\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m/\u001b[0m\u001b[0;36m255\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     16\u001b[0m         \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzero_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     17\u001b[0m         \u001b[0my_pred\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel_training\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx_adv\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-22-c6e624db8243>\u001b[0m in \u001b[0;36mproj_grad_desc\u001b[0;34m(x, y, model, step_size, epsilon, steps, target)\u001b[0m\n\u001b[1;32m      5\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msteps\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m         \u001b[0mcurr\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0madversary\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclone\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdetach\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrequires_grad_\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0madversary\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 7\u001b[0;31m         \u001b[0moutput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcurr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      8\u001b[0m         \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mF\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnll_loss\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mtarget\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m         \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    539\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    540\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 541\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    542\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    543\u001b[0m             \u001b[0mhook_result\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/root/.cache/torch/hub/pytorch_vision_v0.4.2/torchvision/models/resnet.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m    199\u001b[0m         \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmaxpool\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    200\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 201\u001b[0;31m         \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlayer1\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    202\u001b[0m         \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlayer2\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    203\u001b[0m         \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlayer3\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    539\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    540\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 541\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    542\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    543\u001b[0m             \u001b[0mhook_result\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/torch/nn/modules/container.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m     90\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     91\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mmodule\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_modules\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 92\u001b[0;31m             \u001b[0minput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodule\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     93\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     94\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    539\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    540\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 541\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    542\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    543\u001b[0m             \u001b[0mhook_result\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/root/.cache/torch/hub/pytorch_vision_v0.4.2/torchvision/models/resnet.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m    105\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    106\u001b[0m         \u001b[0mout\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconv3\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 107\u001b[0;31m         \u001b[0mout\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbn3\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    108\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    109\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdownsample\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    539\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    540\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 541\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    542\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    543\u001b[0m             \u001b[0mhook_result\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/torch/nn/modules/batchnorm.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m     79\u001b[0m             \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrunning_mean\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrunning_var\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mweight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbias\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     80\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtraining\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrack_running_stats\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 81\u001b[0;31m             exponential_average_factor, self.eps)\n\u001b[0m\u001b[1;32m     82\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     83\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mextra_repr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/torch/nn/functional.py\u001b[0m in \u001b[0;36mbatch_norm\u001b[0;34m(input, running_mean, running_var, weight, bias, training, momentum, eps)\u001b[0m\n\u001b[1;32m   1668\u001b[0m     return torch.batch_norm(\n\u001b[1;32m   1669\u001b[0m         \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mweight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbias\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrunning_mean\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrunning_var\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1670\u001b[0;31m         \u001b[0mtraining\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmomentum\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0meps\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackends\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcudnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0menabled\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1671\u001b[0m     )\n\u001b[1;32m   1672\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mRuntimeError\u001b[0m: CUDA out of memory. Tried to allocate 392.00 MiB (GPU 0; 15.90 GiB total capacity; 14.70 GiB already allocated; 365.88 MiB free; 143.56 MiB cached)"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "U3A9qOsiCHBI",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 289
        },
        "outputId": "ecbbc9a3-b80e-490e-8388-54e5196a8a8b"
      },
      "source": [
        "!nvidia-smi"
      ],
      "execution_count": 64,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Mon Nov 25 00:49:02 2019       \n",
            "+-----------------------------------------------------------------------------+\n",
            "| NVIDIA-SMI 440.33.01    Driver Version: 418.67       CUDA Version: 10.1     |\n",
            "|-------------------------------+----------------------+----------------------+\n",
            "| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
            "| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n",
            "|===============================+======================+======================|\n",
            "|   0  Tesla P100-PCIE...  Off  | 00000000:00:04.0 Off |                    0 |\n",
            "| N/A   36C    P0    31W / 250W |  15915MiB / 16280MiB |      0%      Default |\n",
            "+-------------------------------+----------------------+----------------------+\n",
            "                                                                               \n",
            "+-----------------------------------------------------------------------------+\n",
            "| Processes:                                                       GPU Memory |\n",
            "|  GPU       PID   Type   Process name                             Usage      |\n",
            "|=============================================================================|\n",
            "+-----------------------------------------------------------------------------+\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "60ubhb_ApULg",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 391
        },
        "outputId": "67d6eafe-25e1-4af9-ba75-0f9fa1951b70"
      },
      "source": [
        "import os\n",
        "\n",
        "BASE_PATH = './'\n",
        "if not os.path.exists(BASE_PATH):\n",
        "    os.makedirs(BASE_PATH)\n",
        "DATA_PATH = BASE_PATH + 'tiny_imagenet/'\n",
        "\n",
        "!pwd\n",
        "!ls\n",
        "os.chdir(BASE_PATH)\n",
        "if not os.path.exists(DATA_PATH + 'train.h5'):\n",
        "    !wget https://courses.cs.washington.edu/courses/cse599g1/19au/files/homework2.tar\n",
        "    !tar -xvf homework2.tar\n",
        "    !ls\n",
        "    !rm homework2.tar"
      ],
      "execution_count": 33,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/content\n",
            "data\t   dog.jpg\t\t\t      nanie.jpg       pgd_doggo.jpg\n",
            "doggo.jpg  imagenet1000_clsidx_to_labels.txt  orig_doggo.jpg  sample_data\n",
            "--2019-11-24 23:10:12--  https://courses.cs.washington.edu/courses/cse599g1/19au/files/homework2.tar\n",
            "Resolving courses.cs.washington.edu (courses.cs.washington.edu)... 128.208.1.193, 2607:4000:200:10::c1\n",
            "Connecting to courses.cs.washington.edu (courses.cs.washington.edu)|128.208.1.193|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 1327554560 (1.2G) [application/x-tar]\n",
            "Saving to: ‘homework2.tar’\n",
            "\n",
            "homework2.tar       100%[===================>]   1.24G  19.4MB/s    in 59s     \n",
            "\n",
            "2019-11-24 23:11:11 (21.5 MB/s) - ‘homework2.tar’ saved [1327554560/1327554560]\n",
            "\n",
            "pt_util.py\n",
            "tiny_imagenet/\n",
            "tiny_imagenet/train.h5\n",
            "tiny_imagenet/class_names.txt\n",
            "tiny_imagenet/val.h5\n",
            "data\t   homework2.tar\t\t      orig_doggo.jpg  sample_data\n",
            "doggo.jpg  imagenet1000_clsidx_to_labels.txt  pgd_doggo.jpg   tiny_imagenet\n",
            "dog.jpg    nanie.jpg\t\t\t      pt_util.py\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sie9s4uRry82",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Data loader\n",
        "class H5Dataset(torch.utils.data.Dataset):\n",
        "    def __init__(self, h5_file, transform=None):\n",
        "        # TODO Implement data loading.\n",
        "        f = h5py.File(h5_file, 'r')\n",
        "        self.images = f['images'][...]\n",
        "        self.labels = np.asarray(f['labels'][...], dtype='int64')\n",
        "        print(self.images.shape)\n",
        "        f.close()\n",
        "        self.transform = transform\n",
        "\n",
        "    def __len__(self):\n",
        "        # TODO Implement the length function\n",
        "        return len(self.labels)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        # TODO implement the getitem function\n",
        "        # You should return a tuple of:\n",
        "        #    a torch tensor containing single image in CxHxW format and\n",
        "        #    the label as a single tensor scalar.\n",
        "        data = self.images[idx]\n",
        "        label = self.labels[idx]\n",
        "        if self.transform:\n",
        "            data = self.transform(data)\n",
        "        return (data, label)\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BslBTAdmr7yt",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 85
        },
        "outputId": "28e0de80-6d95-4589-e4e6-d090d313c7ea"
      },
      "source": [
        "import h5py\n",
        "import numpy as np\n",
        "\n",
        "data_train = H5Dataset(DATA_PATH + 'train.h5', transform=preprocess)\n",
        "print(len(data_train))\n",
        "data_test = H5Dataset(DATA_PATH + 'val.h5', transform=preprocess)\n",
        "print(len(data_test))\n"
      ],
      "execution_count": 57,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(100000, 64, 64, 3)\n",
            "100000\n",
            "(8000, 64, 64, 3)\n",
            "8000\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XZPv2rg25nGg",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "preprocess = transforms.Compose([\n",
        "    transforms.ToPILImage(),\n",
        "    transforms.Resize(256),\n",
        "    transforms.CenterCrop(224),\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n",
        "])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RNo4N93YTkxG",
        "colab_type": "text"
      },
      "source": [
        "### Model Description\n",
        "\n",
        "Resnet models were proposed in \"Deep Residual Learning for Image Recognition\".\n",
        "Here we have the 5 versions of resnet models, which contains 5, 34, 50, 101, 152 layers respectively.\n",
        "Detailed model architectures can be found in Table 1.\n",
        "Their 1-crop error rates on imagenet dataset with pretrained models are listed below.\n",
        "\n",
        "| Model structure | Top-1 error | Top-5 error |\n",
        "| --------------- | ----------- | ----------- |\n",
        "|  resnet18       | 30.24       | 10.92       |\n",
        "|  resnet34       | 26.70       | 8.58        |\n",
        "|  resnet50       | 23.85       | 7.13        |\n",
        "|  resnet101      | 22.63       | 6.44        |\n",
        "|  resnet152      | 21.69       | 5.94        |\n",
        "\n",
        "### References\n",
        "\n",
        " - [Deep Residual Learning for Image Recognition](https://arxiv.org/abs/1512.03385)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2o9Wa6DncMf4",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}